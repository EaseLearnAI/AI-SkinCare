
2. 请参考如下示例代码进行调用
import os
from openai import OpenAI

# 请确保您已将 API Key 存储在环境变量 ARK_API_KEY 中
# 初始化Openai客户端，从环境变量中读取您的API Key
client = OpenAI(
    # 此为默认路径，您可根据业务所在地域进行配置
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # 从环境变量中获取您的 API Key
    api_key=os.environ.get("ARK_API_KEY"),
)

# Non-streaming:
print("----- standard request -----")
completion = client.chat.completions.create(
    # 指定您创建的方舟推理接入点 ID，此处已帮您修改为您的推理接入点 ID
    model="ep-20250326233409-44krq",
    messages=[
        {"role": "system", "content": "你是人工智能助手"},
        {"role": "user", "content": "常见的十字花科植物有哪些？"},
    ],
)
print(completion.choices[0].message.content)

# Streaming:
print("----- streaming request -----")
stream = client.chat.completions.create(
    # 指定您创建的方舟推理接入点 ID，此处已帮您修改为您的推理接入点 ID
    model="ep-20250326233409-44krq",
    messages=[
        {"role": "system", "content": "你是人工智能助手"},
        {"role": "user", "content": "常见的十字花科植物有哪些？"},
    ],
    # 响应内容是否流式返回
    stream=True,
)
for chunk in stream:
    if not chunk.choices:
        continue
    print(chunk.choices[0].delta.content, end="")
print()

#API调用指南（deepseek-r1-distill-qwen-7b-250120）

快速开始
快速调用深度推理模型。

其中注意使用下面命令将API Key配置为环境变量ARK_API_KEY。
export ARK_API_KEY="<ARK_API_KEY>"

Curl
Python SDK
OpenAI Python SDK
Go SDK
Java SDK
import os
# 升级方舟 SDK 到最新版本 pip install -U 'volcengine-python-sdk[ark]'
from volcenginesdkarkruntime import Ark

client = Ark(
    # 从环境变量中读取您的方舟API Key
    api_key=os.environ.get("ARK_API_KEY"), 
    # 深度推理模型耗费时间会较长，请您设置较大的超时时间，避免超时，推荐30分钟以上
    timeout=1800,
    )
response = client.chat.completions.create(
    # 替换 <Model> 为模型的Model ID
    model="deepseek-r1-250120",
    messages=[
        {"role": "user", "content": "我要有研究推理模型与非推理模型区别的课题，怎么体现我的专业性"}
    ]
)
# 当触发深度推理时，打印思维链内容
if hasattr(response.choices[0].message, 'reasoning_content'):
    print(response.choices[0].message.reasoning_content)
print(response.choices[0].message.content)

使用说明

减少请求超时失败
您可以使用流式输出或者设置更长超时时间，来减少因为超时导致任务失败的情况。

说明

深度推理模型使用思维链输出内容，导致回复篇幅更长、速率更慢，所以极易因超时导致任务失败。尤其是在非流式输出模式下，任务未完成就会断开连接，既浪费 token 成本，又无法输出内容。

为了避免任务超时导致失败，有下面两种方式：

配置更大的超时时间。特别是非流式输出模型，超时时间timeout推荐设置30分钟以上，您可以观察任务超时触发概率，来进一步调节超时时间。此外，为避免网络中断，还请注意网络链路中的 TCP keep - alive 设置，确保网络连接的稳定性。
推荐使用流式输出。流式输出是一种高效且可靠的方式。您可参考示例代码 流式输出来实现流式调用。同时，建议您避免将应用的输出方式与方舟的输出方式绑定。例如，若您的应用原本处于非流式输出场景，您可以先以流式输出方式获取完整内容，再在应用中一次性输出，这样既能避免任务超时失败，又能满足应用的输出需求。
使用方舟Go SDK时，不管是否使用流式输出，都请将超时时间设置为30分钟以上。


使用批量推理获得更高吞吐
当您的业务需要处理大量的数据，且对于模型返回及时性要求不高，您可使用批量推理获取最低 10B token/天 的配额的使用配额。批量推理支持任务的方式以及类似chat的接口调用方式，使用批量推理，详细说明可以查看批量推理。

提示词优化建议
深度推理模型会自行分析和拆解问题（思维链），与普通模型相比，提示词侧重点有所不同。

提示词除了待解决问题，应该更多补充目标和场景等信息。如使用英语，用Python等语言要求；面向小学生、向领导汇报等阅读对象信息；完成论文写作、完成课题报告、撰写剧本等场景信息；体现我的专业性、获得领导赏识等目标信息。
减少或者避免输入对问题的拆解，如分步骤思考、使用示例等，这样会限制住模型的推理逻辑。
减少使用系统提示词（role: system），所有提示词信息直接通过用户提示词（role: user）来提问。
当碰到deepseek r1模型跳过推理过程，可以在你的提示词前添加下面的内容来输出更高质量内容：
任何输出都要有思考过程，输出内容必须以 "<think>\n\n嗯" 开头。仔细揣摩用户意图，在思考过程之后，提供逻辑清晰且内容完整的回答，可以使用Markdown格式优化信息呈现。\n\n
{你的问题}
当您的问题是数学相关或者涉及到推理类题目，可以使用下面方法来获得步骤明确、逻辑清晰的回答。
请一步一步推理，输出格式为【最终答案：{最终答案} 】\n\n

API 参数说明
下面介绍核心参数的配置，详细 API 字段说明请参见对话(Chat)-文本 API。

输入参数
max_tokens：最大回复长度，默认为 4k，最大可配置为 16k 。思维链最多输出 32k token，不占用最大上下文长度（context window）以及最大回复长度（max_tokens），但是会产生模型输出花费。在实际使用中，如果设置的max_tokens过小，可能导致模型回复内容不完整；若设置过大，可能会增加成本和响应时间 。

如果您在输入的 messages 序列中，传入了reasoning_content，API 会返回 400 错误。因此，请删除 API 响应中的 reasoning_content 字段，再发起 API 请求，方法如快速开始所示。


输出字段
reasoning_content：思维链内容，调用方法见快速开始。请注意，这部分内容也是模型输出的一部分，如果触发深度推理，不管是否打印都会产生 token 花费。

不支持的参数
请注意，以下字段当前 DeepSeek 深度推理模型（deepseek-r1-***）不支持。

字段

类型

传入后行为

stop

String or Array

不支持，忽略不报错。

frequency_penalty

Float

不支持，忽略不报错。

presence_penalty

Float

不支持，忽略不报错。

temperature

Float

不支持，忽略不报错。

top_p

Float

不支持，忽略不报错。

logprobs

Boolean

不支持，报错。

top_logprobs

Integer

不支持，报错。

logit_bias

Object

不支持，报错。


支持的能力
批量推理任务：详细使用方法见创建批量推理任务。
前缀缓存：deepseek-r1-distill-qwen-32b、
deepseek-r1-250120支持，其他模型暂未支持。使用方法见前缀缓存。

Function Calling：详情请参见Function Calling 使用说明。

更多示例

单轮对话
见快速开始。

多轮对话
组合使用系统消息、模型消息以及用户消息，可以实现多轮对话，即根据一个主题进行多次对话。

说明

如果您在输入的 messages 序列中，传入了reasoning_content，API 会返回 400 错误。可参考下例，删除reasoning_content字段内容，详细见附2-工作原理。

Curl
Python SDK
OpenAI Python SDK
Go SDK
Java SDK
curl https://ark.cn-beijing.volces.com/api/v3/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ARK_API_KEY" \
  -d '{ 
    "model": "<Model>",
    "messages": [
        {"role": "user", "content": "推理模型与非推理模型区别"},
        {"role": "assistant", "content": "推理模型主要依靠逻辑、规则或概率等进行分析、推导和判断以得出结论或决策，非推理模型则是通过模式识别、统计分析或模拟等方式来实现数据描述、分类、聚类或生成等任务而不依赖显式逻辑推理。"},
        {"role": "user", "content": "我要有研究推理模型与非推理模型区别的课题，怎么体现我的专业性"}
    ]
  }'

流式输出
随着大模型输出，动态输出内容。无需等待模型推理完毕，即可看到中间输出过程内容，可以缓解用户等待体感（一边输出一边看内容），效果如下所示。

Curl
Python SDK
OpenAI Python SDK
Go SDK
Java SDK
curl https://ark.cn-beijing.volces.com/api/v3/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ARK_API_KEY" \
  -d '{ 
    "model": "<Model>",
    "messages": [
        {"role": "user", "content": "推理模型与非推理模型区别"}
    ],
    "stream": true
  }'



开启联网能力
您可以使用方舟应用的联网插件，为deepseek-r1-250120模型附加上联网能力，让模型能够回答天气、时间等即时的知识问题。方法如下：

获取应用ID：访问应用广场DeepSeek-R1 联网搜索版应用，快速创建联网应用。
应用内容源、返回结果数量等配置项您可根据需要灵活调整。

调用应用：通过应用ID，您即可让模型根据联网结果来分析问题。
Curl
Python SDK
OpenAI Python SDK
Go SDK
JAVA SDK
curl https://ark.cn-beijing.volces.com/api/v3/bots/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $ARK_API_KEY" \
-d '{
    "model": "<BOT_ID>", 
    "messages": [  
        {
            "role": "user", "content": "今天北京天气如何"
        }
    ],
    "stream": true
}'

相关文档
对话(Chat)-文本 API：深度推理 API 参数说明，调用模型的深度推理能力，模型调用中出现错误，可参考深度推理 API 参数说明。

常见问题
并发 RPM 或者 TPM 额度明明有剩余为什么提示限流报错？

附1-应用场景
您可以使用深度推理模型解决复杂问题或者需高准确率的问题，下表是一些常见的应用场景。

场景

场景细分

描述

学术科研领域

论文写作辅助

搜索海量学术文献，并分析相关研究，协助生成论文大纲。对论文中的复杂观点和数据进行分析、总结，帮助您完善论述内容。

复杂问题求解

对于数学、物理等学科中的复杂难题，进行逐步推导和计算，为您提供解题思路和方法。

工作办公场景

文案创作

根据产品特点、目标受众和活动主题，生成富有创意的宣传文案、广告语等；在创作文章时，为您提供灵感，快速搭建文章框架，丰富内容细节。

数据处理与分析

接收大量的数据报表和资料，自动提取关键信息，进行数据整理和分析，生成直观的分析报告，为您的决策提供支持。

教育培训场景

学习辅导

对难题进行详细的讲解和分析，帮助您理解知识点，掌握解题方法。

课程设计

根据教学目标、学生的学习水平和课程大纲，提供教学内容的设计思路、案例素材等，辅助教师打造更优质的课程。


附2-工作原理
深度推理模型除了提问（Question）和回答（Answer）外，还会输出思维链内容（COT）。思维链内容展现的是模型处理问题的过程，包括将问题拆分为多个问题进行处理，生成多种回复综合得出更好回答等过程。但是这个内容不会被拼接到上下文中，如下图所示。


在控制模型输入输出以及计费时会用到不同的概念，他们作用的内容会有所不同，如下图所示。

计费逻辑：思维链和回答内容长度会作为输出计费项进行计费，问题会作为输入计费项进行计费。
内容长度限制：计算思维链内容长度由最大思维链长度单独限制；问题和回答内容合并作为上下文，受最大上下文长度限制；回答内容会有额外的max token来控制模型输出回答的长度限制。
